{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. csv 파일 원본에서 필요한 부분만 JSONL 파일 형식으로 저장(Write)"
      ],
      "metadata": {
        "id": "blaXVvPt4Hxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "import random\n",
        "\n",
        "def read_conversation_log(file_path):\n",
        "    conversations = {}\n",
        "    current_conversation = []\n",
        "    current_id = None\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            conversation_id, speaker, message = row\n",
        "            if current_id != conversation_id:\n",
        "                if current_conversation:\n",
        "                    conversations[current_id] = current_conversation\n",
        "                current_id = conversation_id\n",
        "                current_conversation = []\n",
        "            current_conversation.append((speaker, message))\n",
        "\n",
        "    if current_conversation:\n",
        "        conversations[current_id] = current_conversation\n",
        "\n",
        "    return conversations\n",
        "\n",
        "def read_learning_data(file_path):\n",
        "    data = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        headers = reader.fieldnames\n",
        "        conversation_id_column = 'Conversation ID'\n",
        "\n",
        "        # 'Conversation ID' 컬럼이 없으면 첫 번째 컬럼을 사용\n",
        "        if 'Conversation ID' not in headers:\n",
        "            conversation_id_column = headers[0]\n",
        "            print(f\"Warning: 'Conversation ID' column not found. Using '{conversation_id_column}' instead.\")\n",
        "\n",
        "        for row in reader:\n",
        "            try:\n",
        "                conversation_id = row[conversation_id_column]\n",
        "                data[conversation_id] = row\n",
        "            except KeyError as e:\n",
        "                print(f\"Error processing row: {row}\")\n",
        "                print(f\"KeyError: {e}\")\n",
        "                continue\n",
        "    return data\n",
        "\n",
        "def create_jsonl_entry(conversation, learning_data):\n",
        "    customer_messages = [msg for speaker, msg in conversation if speaker == 'Customer']\n",
        "    customer_dialogue = ' '.join(customer_messages)\n",
        "\n",
        "    detected_lie = learning_data.get('Detected Lie', 'None')\n",
        "    confidence = learning_data.get('Detection Confidence', '0')\n",
        "\n",
        "    if detected_lie != 'None' and int(confidence) > 0:\n",
        "        is_lie = True\n",
        "        analysis = f\"The customer made a false statement: {detected_lie}. Confidence: {confidence}%\"\n",
        "    else:\n",
        "        is_lie = False\n",
        "        analysis = \"No false statements detected in the customer's dialogue.\"\n",
        "\n",
        "    user_content = f\"Customer's statements: {customer_dialogue}\"\n",
        "\n",
        "    if is_lie:\n",
        "        assistant_content = f\"\"\"Based on the analysis of the customer's statements, a potential false statement has been detected:\n",
        "\n",
        "Detected False Statement: {detected_lie}\n",
        "Confidence: {confidence}%\n",
        "\n",
        "Analysis:\n",
        "{analysis}\n",
        "\n",
        "This discrepancy may indicate an attempt by the customer to misrepresent their situation. It is recommended that the underwriter investigate this matter further to ensure accurate risk assessment.\"\"\"\n",
        "    else:\n",
        "        assistant_content = f\"\"\"Based on the analysis of the customer's statements, no false statements were detected.\n",
        "\n",
        "Analysis:\n",
        "{analysis}\n",
        "\n",
        "The customer's statements appear to be consistent and truthful. However, it is always recommended to verify key information through standard underwriting procedures.\"\"\"\n",
        "\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"Your goal is to detect any false statements made by the insurance applicant during the underwriting conversation. After detecting these inconsistencies, you will draft an analytical report outlining the detected discrepancies and provide this report to the underwriter for further review. The report should be based on the dialogue input provided by the user.\"},\n",
        "            {\"role\": \"user\", \"content\": user_content},\n",
        "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    conversations = read_conversation_log('conversation_log.csv')\n",
        "    learning_data = read_learning_data('learning_data.csv')\n",
        "\n",
        "    with open('train.jsonl', 'w', encoding='utf-8') as outfile:\n",
        "        for conv_id, conversation in conversations.items():\n",
        "            if conv_id in learning_data:\n",
        "                entry = create_jsonl_entry(conversation, learning_data[conv_id])\n",
        "                json.dump(entry, outfile, ensure_ascii=False)\n",
        "                outfile.write('\\n')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "print(\"train.jsonl 생성 완료\")"
      ],
      "metadata": {
        "id": "H10ySs2V_br-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fine-Tuning 수행"
      ],
      "metadata": {
        "id": "yxS4mRvB5Imb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# openai 라이브러리 인스톨\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "Wm2D2Fhe5INw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (1) 로컬에 저장된 학습데이터 train.jsonl 업로드\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI( # 클라이언트 인스턴스 생성\n",
        "\n",
        "    api_key = \"API KEY REQUIRED\"\n",
        "\n",
        ")\n",
        "\n",
        "file = client.files.create(\n",
        "\n",
        "    file = open('train.jsonl', 'rb'), # 이진파일로 file에 읽어들임\n",
        "\n",
        "\n",
        "    purpose = 'fine-tune' # 파인튜닝 목적임을 명기\n",
        "\n",
        ")\n",
        "\n",
        "print(file)"
      ],
      "metadata": {
        "id": "5oA5bZ0y5Qj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 위 소스코드 실행 결과의 id를 아래 소스코드의 training_file에 작성해줘야 한다."
      ],
      "metadata": {
        "id": "B048yx8z5wRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (2) Fine-Tuning Job 생성하기\n",
        "\n",
        "client.fine_tuning.jobs.create(\n",
        "\n",
        "    training_file = file.id,\n",
        "\n",
        "    model = 'gpt-3.5-turbo-1106', # 사용할 모델명 기입\n",
        "\n",
        "    suffix = \"underwriter-support-model\", # 모델명 설정\n",
        "\n",
        "    # n_epochs = 3, # 에포크 수\n",
        "\n",
        "    # learning_rate_multiplier = 0.1 # 학습률 배수\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "xxYTddhc5idG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 위 소스코드 실행 결과의 id를 아래 소스코드의 retrieve 함수로 넘겨줘야 한다.\n",
        "- 실행 결과에서, status = 'succeeded' 여부를 확인한다.\n",
        "- OpenAI 콘솔의 Fine-Tuning UI를 통해, 모델 학습 현황을 확인하는 것도 가능하다."
      ],
      "metadata": {
        "id": "NWfjIpBb6oU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (3) 학습 완료여부(Status) 확인\n",
        "\n",
        "client.fine_tuning.jobs.list()\n",
        "# client.fine_tuning.jobs.list(limit = 10) # API 요청을 통해 한 번에 조회가능한 파인튜닝 작업의 개수 제한\n",
        "\n",
        "client.fine_tuning.jobs.retrive(\"ftjob-파인튜닝 완료된 모델의 코드\")"
      ],
      "metadata": {
        "id": "nXr4FHHK6Jk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Fine-Tuned Model 직접 사용하기\n",
        "- messages의 system(key)에는 지정했던 instruction 삽입\n",
        "- messages의 user(key)에는 파인튜닝된 ChatGPT(Underwriter)에게 상담내용 예시 입력받으면 적절한 언더라이팅 솔루션 제시\n",
        "\n",
        "<br><br>\n",
        "\n",
        "### ⚠️주의사항 : 함수를 한 번 사용할 때마다 일정 토큰이 소비되므로 주의하여 실행"
      ],
      "metadata": {
        "id": "H-IO3t5t7Hq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = input(\"고객(피보험자)의 담화내용(영어) 입력 : \")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "\n",
        "    # 파인튜닝에 성공한 모델명 명시\n",
        "    model = \"ft:gpt-3.5-turbo-1106:personal::뒤에 뜨는 코드\",\n",
        "\n",
        "    messages = [\n",
        "        {\"role\" : \"system\", \"content\" : instruction},\n",
        "        {\"role\" : \"user\", \"content\" : question}\n",
        "    ]\n",
        "\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content.strip())"
      ],
      "metadata": {
        "id": "lgnUWB9G7MWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "# 일괄실행"
      ],
      "metadata": {
        "id": "8sVXO9EkUz5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# openai 라이브러리 인스톨\n",
        "!pip install openai\n",
        "\n",
        "# 라이브러리 import\n",
        "import csv\n",
        "import json\n",
        "import random\n",
        "from openai import OpenAI\n",
        "\n",
        "def read_conversation_log(file_path):\n",
        "    conversations = {}\n",
        "    current_conversation = []\n",
        "    current_id = None\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            conversation_id, speaker, message = row\n",
        "            if current_id != conversation_id:\n",
        "                if current_conversation:\n",
        "                    conversations[current_id] = current_conversation\n",
        "                current_id = conversation_id\n",
        "                current_conversation = []\n",
        "            current_conversation.append((speaker, message))\n",
        "\n",
        "    if current_conversation:\n",
        "        conversations[current_id] = current_conversation\n",
        "\n",
        "    return conversations\n",
        "\n",
        "def read_learning_data(file_path):\n",
        "    data = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        headers = reader.fieldnames\n",
        "        conversation_id_column = 'Conversation ID'\n",
        "\n",
        "        # 'Conversation ID' 컬럼이 없으면 첫 번째 컬럼을 사용\n",
        "        if 'Conversation ID' not in headers:\n",
        "            conversation_id_column = headers[0]\n",
        "            print(f\"Warning: 'Conversation ID' column not found. Using '{conversation_id_column}' instead.\")\n",
        "\n",
        "        for row in reader:\n",
        "            try:\n",
        "                conversation_id = row[conversation_id_column]\n",
        "                data[conversation_id] = row\n",
        "            except KeyError as e:\n",
        "                print(f\"Error processing row: {row}\")\n",
        "                print(f\"KeyError: {e}\")\n",
        "                continue\n",
        "    return data\n",
        "\n",
        "def create_jsonl_entry(conversation, learning_data):\n",
        "    customer_messages = [msg for speaker, msg in conversation if speaker == 'Customer']\n",
        "    customer_dialogue = ' '.join(customer_messages)\n",
        "\n",
        "    detected_lie = learning_data.get('Detected Lie', 'None')\n",
        "    confidence = learning_data.get('Detection Confidence', '0')\n",
        "\n",
        "    if detected_lie != 'None' and int(confidence) > 0:\n",
        "        is_lie = True\n",
        "        analysis = f\"The customer made a false statement: {detected_lie}. Confidence: {confidence}%\"\n",
        "    else:\n",
        "        is_lie = False\n",
        "        analysis = \"No false statements detected in the customer's dialogue.\"\n",
        "\n",
        "    user_content = f\"Customer's statements: {customer_dialogue}\"\n",
        "\n",
        "    if is_lie:\n",
        "        assistant_content = f\"\"\"Based on the analysis of the customer's statements, a potential false statement has been detected:\n",
        "\n",
        "Detected False Statement: {detected_lie}\n",
        "Confidence: {confidence}%\n",
        "\n",
        "Analysis:\n",
        "{analysis}\n",
        "\n",
        "This discrepancy may indicate an attempt by the customer to misrepresent their situation. It is recommended that the underwriter investigate this matter further to ensure accurate risk assessment.\"\"\"\n",
        "    else:\n",
        "        assistant_content = f\"\"\"Based on the analysis of the customer's statements, no false statements were detected.\n",
        "\n",
        "Analysis:\n",
        "{analysis}\n",
        "\n",
        "The customer's statements appear to be consistent and truthful. However, it is always recommended to verify key information through standard underwriting procedures.\"\"\"\n",
        "\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"Your goal is to detect any false statements made by the insurance applicant during the underwriting conversation. After detecting these inconsistencies, you will draft an analytical report outlining the detected discrepancies and provide this report to the underwriter for further review. The report should be based on the dialogue input provided by the user.\"},\n",
        "            {\"role\": \"user\", \"content\": user_content},\n",
        "            {\"role\": \"assistant\", \"content\": assistant_content}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "\n",
        "conversations = read_conversation_log('conversation_log.csv')\n",
        "learning_data = read_learning_data('learning_data.csv')\n",
        "\n",
        "with open('train.jsonl', 'w', encoding='utf-8') as outfile:\n",
        "    for conv_id, conversation in conversations.items():\n",
        "        if conv_id in learning_data:\n",
        "            entry = create_jsonl_entry(conversation, learning_data[conv_id])\n",
        "            json.dump(entry, outfile, ensure_ascii=False)\n",
        "            outfile.write('\\n')\n",
        "print(\"train.jsonl 생성 완료\")\n",
        "\n",
        "# (1) 로컬에 저장된 학습데이터 train.jsonl 업로드\n",
        "client = OpenAI( # 클라이언트 인스턴스 생성\n",
        "\n",
        "    api_key = \"API KEY REQUIRED\"\n",
        "\n",
        ")\n",
        "\n",
        "file = client.files.create(\n",
        "\n",
        "    file = open('train.jsonl', 'rb'), # 이진파일로 file에 읽어들임\n",
        "\n",
        "\n",
        "    purpose = 'fine-tune' # 파인튜닝 목적임을 명기\n",
        "\n",
        ")\n",
        "print(file)\n",
        "\n",
        "# (2) Fine-Tuning Job 생성하기\n",
        "\n",
        "client.fine_tuning.jobs.create(\n",
        "\n",
        "    training_file = file.id,\n",
        "\n",
        "    model = 'gpt-3.5-turbo-1106', # 사용할 모델명 기입\n",
        "\n",
        "    suffix = \"underwriter-support-model\", # 모델명 설정\n",
        "\n",
        "    # n_epochs = 3, # 에포크 수\n",
        "\n",
        "    # learning_rate_multiplier = 0.1 # 학습률 배수\n",
        "\n",
        ")\n",
        "\n",
        "# (3) 학습 완료여부(Status) 확인\n",
        "\n",
        "client.fine_tuning.jobs.list()\n",
        "# client.fine_tuning.jobs.list(limit = 10) # API 요청을 통해 한 번에 조회가능한 파인튜닝 작업의 개수 제한\n",
        "\n",
        "client.fine_tuning.jobs.retrive(\"ftjob-파인튜닝 완료된 모델의 코드\")"
      ],
      "metadata": {
        "id": "ZbbvaiOJU1Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = input(\"고객(피보험자)의 담화내용(영어) 입력 : \")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "\n",
        "    # 파인튜닝에 성공한 모델명 명시\n",
        "    model = \"ft:gpt-3.5-turbo-1106:personal::뒤에 뜨는 코드\",\n",
        "\n",
        "    messages = [\n",
        "        {\"role\" : \"system\", \"content\" : instruction},\n",
        "        {\"role\" : \"user\", \"content\" : question}\n",
        "    ]\n",
        "\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content.strip())"
      ],
      "metadata": {
        "id": "oQxF9PMNVZoc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}